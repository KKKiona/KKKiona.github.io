[{"title":"VPS服务器搭建SSR/SS","date":"2019-04-06T06:02:53.301Z","path":"2019/04/06/VPS服务器搭建SSR_SS/","text":"实在是忍受不了国内博客论坛千篇一律的抄袭现象，决定去翻阅一下国外的资料。但又因为国内现成的VPN基本全被封杀，即便是有几个可以用的也会收费并且非常不稳定，于是决定自己动手打一个梯子。服务器：vultr（比较实惠、稳定的国外服务器）工具：xshell、shadowstocks、bbr、代理服务设置工具（下边会给出链接）首先是搞到服务器，去vultr注册账号。https://www.vultr.com/填写邮箱和密码，会给你发个邮件激活账号。先进性账户充值，充值最低10美元起。倒数2、3分别是微信和支付宝，扫码支付就可以了。 选择服务器选一个适合你的（延迟低的服务器）推荐日本和美国。如果不清楚哪个延迟低可以去这里测试一下http://ga-us-ping.vultr.com/ms越低越好。 选择系统和机器由于我使用kali较多所以我选了Debian，无特殊需求选默认的CentOS就可以了。机器的话如果仅仅用来fan墙，最便宜的就足够了。 后面的选项对此次的目的来说没有意义，因此全部采用的默认，具体功能自行了解。最后点击下方deploy now。 这样我们就有了一台自己的海外服务器。点击你的服务器进入详页可以看到你服务器IP和用户名、密码。后边会用到。下载xshell 链接：https://pan.baidu.com/s/11FRT_RPIVZjyvMkjx-zYbA 提取码：ffki安装完毕后新建连接。名字随意，主机号就是你申请的副武器的IP，端口一般默认22，可以自己修改。点击确定并连接主机。输入服务器的用户名密码。连接服务器。如图表示连接成功。 输入 1wget -N --no-check-certificate https://raw.githubusercontent.com/hombo125/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh 如果失败则先输入下面的指令再次运行上面的指令 1yum -y install wget 运行完成后出现如下提示，输入数字1安装ssr服务端输入端口和账号密码，端口的可选范围为60~65535。选择加密方式，输入对应数字即可，下面机几个操作的选择取决于个人，作者均采用默认设置。协议插件设置混淆（通过代理玩游戏推荐第一个速度快一些）设备限制空（回车）默认无限制，需要对访问速度加以限制的自己输入速率上限就行。下载文件安装完成后会显示相关信息，记得保存。若不慎关闭可以输入bash ssr.sh进入管理界面根据提示查看或者修改信息。重启服务器，可以再界面输入reboot也可以在你的vultr管理界面里重启。 安装BBR加速器，输入如下指令 123wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.shchmod +x bbr.sh./bbr.sh 按任意键安装。完成后提示重启服务器选择Y或手动重启。重启后输入lsmod | grep bbr查看是否安装成功。如上图安装成功。 下载代理设置工具 链接：https://pan.baidu.com/s/1_68i7TxfdT4lv3Kupge2wQ 提取码：z8jh 把之前保存的ssr配置信息填上。 设置浏览器的代理服务，地址127.0.0.1端口1080。使用全局模式所有浏览器就都可以使用代理服务了。Chrome可以使用自己的SwitchyOmega插件来定制自己的代理服务，但是这个插件的官方链接也被墙了，这里给出一个离线下载https://chrome-extension-downloader.com/。使用很简单实在不懂就Baidu一下吧。 至此vps服务器搭建ssr/ss就完成了，效果。","comments":true,"tags":[]},{"title":"SVG 图片验证码解码","date":"2019-04-05T13:28:43.169Z","path":"2019/04/05/SVG 图片验证码解码/","text":"环境：Python 3外部依赖：requests、bs4、BeautifulSoup、base64先看题目：题目类似于公式计算的那道题只是公式变成了图片验证码，所以关于爬虫和网页请求请看https://kkkiona.github.io/2019/04/05/Python%20%E7%AE%80%E5%8D%95%E7%9A%84%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96%E5%92%8C%E6%8F%90%E4%BA%A4%E8%A1%A8%E5%8D%95/下面是解题过程：查看网页代码是用SVG制作的图片并且采用了base64进行了加密。由于作者有一些canvas的使用经验，因此对SVG也有所了解，这里简单说一下。SVG是一种适量绘制技术，与HTML5的canvas技术相比最大的区别就是，canvas有与之关联的JavaScript API 但是SVG是通过xml来控制的。SVG的绘制质量运行速率也不如canvas，好处就是易于控制。后期我会发一篇详细的介绍。回归主题，把这个图片单独拿出来看一下就是这个样子，可以看到URL的base64后面跟了一大堆乱码，这就是加密的xml文档，F12可以看到解密以后的样子，当然也可以通过各种解密工具查看。那么我们就有两种思路，第一种就是像这样靠浏览器来解码然后抓解码后的内容，如果这么做就完全变成抓包游戏了，所以我更推荐第二种本地解码。先抓取题目的原始页面，然后通过简单的字符切割获取那段乱码字符串，方法见上面链接，这里只给出代码： 123response = requests.get(url)soup = BeautifulSoup(response.text, &quot;html.parser&quot;)src = soup.find(&quot;img&quot;).get(&quot;src&quot;).split(&apos;,&apos;)[1] 接下来重点就是解码，使用base64包解码。 12import base64enc = base64.b64decode(src) 可以把解码后的内容打印输出看一下，会发现跟浏览器的显示一样这里就不放图片了。下面的内容就很简单了，上方连接使用的关键字提取方法一样，只需要把text标签里面的内容提取出来组和并提交就可以了。但是结果会发现提交的结果是Wrong。仔细观察图片和浏览器里的解码后的代码会发现text中字符的排列顺序并不是想要的顺序，如上代码的前两个字符是D u 但是图片是t S。原来图片中的字符顺序是根据x轴的位置决定的，那么我们就需要对获取的文本重新排序了。 鉴于我的Python学的跟shi一样（正在重学）所以以下内容仅供参考，比我方法好的比比皆是。 将解码的文件转换为BeautifulSoup对象 1soup = BeautifulSoup(enc, &quot;xml&quot;) 查找所有text标签并返回tag对象，将tag对象的x轴的值转换为int类，并且连同text里面的字符逐个付给一个空的list列表。 1234List = []for tag in soup.find_all(&apos;text&apos;): List.append([int(tag[&apos;x&apos;]), tag.string]) tag是soup.find()返回的对象类型，具体关于BeautifuSsoup的使用可以参考官文https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/ 接下来只需要使用list的排序功能就可以了，由于我把x的值作为了第一个属性所以无需指定排序关键字，放到第二个的话需要手动指定，步骤很简单自行搜索即可。 1List.sort() 可以输出看一下字符的顺序按照离x轴的距离拍好了。最后取出里面的字符组合起来就可以了。 1234getString = &quot;&quot;for c in List: getString += c[1] 剩下的就是提交了这里就不重复了。最后结果OK，最后完整代码 123456789101112131415161718192021222324252627282930313233import requestsfrom bs4 import BeautifulSoupimport base64url = &quot;http://example.com/&quot;response = requests.get(url)Cookies = response.cookiessoup = BeautifulSoup(response.text, &quot;html.parser&quot;)src = soup.find(&quot;img&quot;).get(&quot;src&quot;).split(&apos;,&apos;)[1]enc = base64.b64decode(src)soup = BeautifulSoup(enc, &quot;xml&quot;)List = []for tag in soup.find_all(&apos;text&apos;): List.append([int(tag[&apos;x&apos;]), tag.string])List.sort()getString = &quot;&quot;for c in List: getString += c[1]response = requests.get(url + &quot;?code=&quot; + getString, cookies = Cookies)print(response.text) 最最后附上Python的其他编码解码方式。转载：https://blog.csdn.net/five3/article/details/83626446","comments":true,"tags":[]},{"title":"Python 简单的网页抓取和提交表单","date":"2019-04-05T12:43:39.996Z","path":"2019/04/05/Python 简单的网页抓取和提交表单/","text":"环境说明：Python 3外部依赖：requests、bs4、BeautifulSoup先看题目 题目很简单，就是限时计算给出的算式并提交。方法一、计算器一台。方法很简单只需要在1.5s内计算出结果再提交就行了，大约几千左右的APM就够了。方法二、使用网络爬虫。爬虫的话自然使用Python是最方便不过的，大体思路就是抓取公式-&gt;计算-&gt;提交。既然是做题那么首先第一步肯定是化简，做题最讨厌的不是不会做而是把1+1做成∞+∞。我们先多刷新几次页面看一下。可以很明显看到公式采用的是x1+x2*x3-x4的固定格式。那这就简单多了，不需要判断运算符也不用修改运算顺序，直接省去了一个heap的工作量。再来看一下代码。可以看到表单使用Get传值，传递对象是自己。GET使用URL传递参数，来测试一下URL。提交前：提交后：可以看到answer的值被脚本通过URL捕获并显示相关内容。现在的工作具体为抓取抓取公式-&gt;计算-&gt;生成URL-&gt;提交先来完成前两步工作：新建一个py文件，引入抓包用的requests和分析HTML结构的bs4. 12import requestsfrom bs4 import BeautifulSoup 向指定页面发送GET请求并获取响应 1response = requests.get(&quot;http://example.com/&quot;) 得到的response响应可以以content（二进制流）、text（文本）等多种格式输出，作为文本的时候还可以指定编码格式。现以text格式输出一下看看结果。 1print(response.text) 毫无疑问我们成功抓到了页面内容。下面需要获取公式。 12345678soup = BeautifulSoup(response.text, &quot;html.parser&quot;)getString = soup.find(id=&quot;exp&quot;).stringSum = int(getString.split()[2])Sum *= int(getString.split()[4])Sum += int(getString.split()[0])Sum -= int(getString.split()[6]) 先把获取的文本转成BeautifulSoup对象，通过字符串截取和强制转换变成单个数字并计算。由于已经确定运算符和运算规则这里就容易很多。然后把答案插入到URL并发送GET请求 1response = requests.get(&quot;http://example.com/calculator/?answer=&quot; + &apos;%d&apos; % Sum) 然后你会发现提示问题页面还未建立，没有问题何来答案。简述一下HTTP请求的规则：HTTP请求的三次握手客户端向服务端发送syn=1，seq=client请求的ID;服务端向客户端发送syn=1,seq=服务端请求的ID,ack=客户端请求的ID+1;客户端向服务端发送syn=0,seq=客户端请求的ID+1,ack=服务端请求的ID+1,data\\data…所以简单来说就是HTTP向服务器发送请求时会有一个专属ID来让服务器和客户端保持连接，这个ID存在于cookies里面。如果不连同cookies发送给服务器，那么服务器就会认为是两个不同的客户发送的请求从而出现上面的错误。所以我们要做的是连同cookies一起发送。先从response里面获取cookies并保存。 1Cookies = response.cookies 修改二次请求 1response = requests.get(&quot;http://example.com/?answer=&quot; + &apos;%d&apos; % Sum, cookies=Cookies) 再来看一下结果成功！总结：这道题比较基础，简单的使用了Python的爬虫功能，主要是学习了HTTP请求的过程，关键在于cookies的传递。 忘记放出完整代码了，补上！ 123456789101112131415161718import requestsfrom bs4 import BeautifulSoupresponse = requests.get(&quot;http://123.207.149.64:23331/calculator/&quot;)Cookies = response.cookiessoup = BeautifulSoup(response.text, &quot;html.parser&quot;)getString = soup.find(id=&quot;exp&quot;).stringSum = int(getString.split()[2])Sum *= int(getString.split()[4])Sum += int(getString.split()[0])Sum -= int(getString.split()[6])response = requests.get(&quot;http://123.207.149.64:23331/calculator/?answer=&quot; + &apos;%d&apos; % Sum, cookies=Cookies)print(response.text) 后面打算写一篇关于POST传递的使用方式和详细的HTTP请求的相关知识","comments":true,"tags":[]},{"title":"4-5日随笔","date":"2019-04-05T01:41:51.171Z","path":"2019/04/05/4-5/","text":"清明回老家，早上先把题目看了一下，还是一道网络爬虫的题目，不过这次待解密的数据是用svg+xml绘制的验证码刚好之前接触canvas的时候了解过svg，先手动解密了一下，晚上回来写脚本。","comments":true,"tags":[{"name":"随笔","slug":"随笔","permalink":"https://kkkiona.github.io/tags/随笔/"}]},{"title":"关于Python的使用","date":"2019-04-04T08:35:03.172Z","path":"2019/04/04/关于Python的使用/","text":"今天遇到了一道网络爬虫的题目，于是便重新拾起了N年前自己学习的Python 2，下面就说一下Python的相关问题。首先关于Python版本的问题，虽然Python 2 ，Python 3 都在更新，而且据统计显示有70%以上的作者仍在使用Python 2，但是真的非常强烈推荐Python 3。Python 3 不仅仅是对语法做了优化，而且对许多库做了改进、合并。比如Python 2 的urllib和urllib2 在Python 3 中合并为了urllib（其实都不如request）。总之Python 3 是非常值得使用的。下面是一些安装问题，你可以去官网下载Python 3并安装，https://www.python.org/ Python 3 的Windows安装包是可以勾选path环境变量的无需手动设置。但相比这样我更推荐使用 anaconda 集成包，里面包含了许多科学包及其依赖，同样是一键安装并且可以自动配置环境变量，https://www.anaconda.com/ 提供了 Python 2，Python 3 两个版本。关于编辑器，新手可以使用Thonny，十分小巧简洁的Python 编辑器，没有复杂的工程模块，打开软件写就完了，并且自带了Python环境无需提前安装Python即可使用，也可以自己配置解释器。缺点是功能相对较弱。https://thonny.org/比较常见的是pyCharm，操作模式类似于eclipse，功能强大代，码高亮，自动填写做的很好。项目管理很方便，可以跨平台。可以很方便的管理库文件，添加外部依赖等等。提供了免费及收费两个版本。没什么大缺点，就是对外部包引入的搜索方式有时候会让人头大。明明项目里都显示有package了就是import不进来。https://www.jetbrains.com/pycharm/download/#section=windows另外推荐一个交互式笔记本，Jupyter notebook（又称IPython notebook）支持运行超过40种编程语言。安装教程十分简单。跨平台，实时代码编辑。缺点，关键词高亮很差，语法检测很弱，有时候错误语法照样可以运行。https://jupyter.org/对于热衷文本编辑器的人员可以使用sublime，另外C、Java程序员可以使用vs和eclipse。Linux推荐vim。关于同时使用Python 2 和 Python 3，如果确实需要两个环境的话。可以参考这篇文章https://www.cnblogs.com/zhengyihan1216/p/6011640.html简述一下就是安装第一个Python的时候记得把Python.exe改个别名，并且使用pip的时候要对应Python版本，最后分别添加path就可以了。下一篇介绍如何简单的使用 requests package抓取页面和表单提交。","comments":true,"tags":[]},{"title":"First Article","date":"2019-04-03T12:24:54.992Z","path":"2019/04/03/first blog/","text":"早上看了一下之前的Github实在是不忍直视还是重新制作一个吧。使用的是hexo搭建的博客，步骤比较简单。但是在themes选择上真的是强迫症患者的噩梦。偏偏就喜欢上了一个已经停止维护的yilia主题，尝试了无数次修改buffer大小之后依然无法使用HTTP下载，还是ssh吧。下载下来之后更是bug一大堆，比如tag。修改了半天总算是基本正常了。 既然是新做的博客必须好好搞一下，从网易云拉了音乐，另外搞了一下午的图灵机器人，而且失败了。。其实hexo 是自带Live2D功能的，但是想做一个高大上的带交友功能的机器人。然而node.js不熟悉挂接图灵接口的时候就是不行，最后无奈先用自带的吧。明天开始研究信息安全。有时间再搞图灵。。","comments":true,"tags":[{"name":"随笔","slug":"随笔","permalink":"https://kkkiona.github.io/tags/随笔/"}]},{"title":"Hello World","date":"2019-04-03T03:37:59.399Z","path":"2019/04/03/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","comments":true,"tags":[]}]