[{"title":"Python 简单的网页抓取和提交表单","date":"2019-04-05T12:43:39.996Z","path":"2019/04/05/Python 简单的网页抓取和提交表单/","text":"环境说明：Python 3外部依赖：requests、bs4、BeautifulSoup先看题目 题目很简单，就是限时计算给出的算式并提交。方法一、计算器一台。方法很简单只需要在1.5s内计算出结果再提交就行了，大约几千左右的APM就够了。方法二、使用网络爬虫。爬虫的话自然使用Python是最方便不过的，大体思路就是抓取公式-&gt;计算-&gt;提交。既然是做题那么首先第一步肯定是化简，做题最讨厌的不是不会做而是把1+1做成∞+∞。我们先多刷新几次页面看一下。可以很明显看到公式采用的是x1+x2*x3-x4的固定格式。那这就简单多了，不需要判断运算符也不用修改运算顺序，直接省去了一个heap的工作量。再来看一下代码。可以看到表单使用Get传值，传递对象是自己。GET使用URL传递参数，来测试一下URL。提交前：提交后：可以看到answer的值被脚本通过URL捕获并显示相关内容。现在的工作具体为抓取抓取公式-&gt;计算-&gt;生成URL-&gt;提交先来完成前两步工作：新建一个py文件，引入抓包用的requests和分析HTML结构的bs4. 12import requestsfrom bs4 import BeautifulSoup 向指定页面发送GET请求并获取响应 1response = requests.get(&quot;http://example.com/&quot;) 得到的response响应可以以content（二进制流）、text（文本）等多种格式输出，作为文本的时候还可以指定编码格式。现以text格式输出一下看看结果。 1print(response.text) 毫无疑问我们成功抓到了页面内容。下面需要获取公式。 12345678soup = BeautifulSoup(response.text, &quot;html.parser&quot;)getString = soup.find(id=&quot;exp&quot;).stringSum = int(getString.split()[2])Sum *= int(getString.split()[4])Sum += int(getString.split()[0])Sum -= int(getString.split()[6]) 先把获取的文本转成BeautifulSoup对象，通过字符串截取和强制转换变成单个数字并计算。由于已经确定运算符和运算规则这里就容易很多。然后把答案插入到URL并发送GET请求 1response = requests.get(&quot;http://example.com/calculator/?answer=&quot; + &apos;%d&apos; % Sum) 然后你会发现提示问题页面还未建立，没有问题何来答案。简述一下HTTP请求的规则：HTTP请求的三次握手客户端向服务端发送syn=1，seq=client请求的ID;服务端向客户端发送syn=1,seq=服务端请求的ID,ack=客户端请求的ID+1;客户端向服务端发送syn=0,seq=客户端请求的ID+1,ack=服务端请求的ID+1,data\\data…所以简单来说就是HTTP向服务器发送请求时会有一个专属ID来让服务器和客户端保持连接，这个ID存在于cookies里面。如果不连同cookies发送给服务器，那么服务器就会认为是两个不同的客户发送的请求从而出现上面的错误。所以我们要做的是连同cookies一起发送。先从response里面获取cookies并保存。 1Cookies = response.cookies 修改二次请求 1response = requests.get(&quot;http://example.com/?answer=&quot; + &apos;%d&apos; % Sum, cookies=Cookies) 再来看一下结果成功！总结：这道题比较基础，简单的使用了Python的爬虫功能，主要是学习了HTTP请求的过程，关键在于cookies的传递。 忘记放出完整代码了，补上！ 123456789101112131415161718import requestsfrom bs4 import BeautifulSoupresponse = requests.get(&quot;http://123.207.149.64:23331/calculator/&quot;)Cookies = response.cookiessoup = BeautifulSoup(response.text, &quot;html.parser&quot;)getString = soup.find(id=&quot;exp&quot;).stringSum = int(getString.split()[2])Sum *= int(getString.split()[4])Sum += int(getString.split()[0])Sum -= int(getString.split()[6])response = requests.get(&quot;http://123.207.149.64:23331/calculator/?answer=&quot; + &apos;%d&apos; % Sum, cookies=Cookies)print(response.text) 后面打算写一篇关于POST传递的使用方式和详细的HTTP请求的相关知识","comments":true,"tags":[]},{"title":"4-5日随笔","date":"2019-04-05T01:41:51.171Z","path":"2019/04/05/4-5/","text":"清明回老家，早上先把题目看了一下，还是一道网络爬虫的题目，不过这次待解密的数据是用svg+xml绘制的验证码刚好之前接触canvas的时候了解过svg，先手动解密了一下，晚上回来写脚本。","comments":true,"tags":[{"name":"随笔","slug":"随笔","permalink":"https://kkkiona.github.io/tags/随笔/"}]},{"title":"关于Python的使用","date":"2019-04-04T08:35:03.172Z","path":"2019/04/04/关于Python的使用/","text":"今天遇到了一道网络爬虫的题目，于是便重新拾起了N年前自己学习的Python 2，下面就说一下Python的相关问题。首先关于Python版本的问题，虽然Python 2 ，Python 3 都在更新，而且据统计显示有70%以上的作者仍在使用Python 2，但是真的非常强烈推荐Python 3。Python 3 不仅仅是对语法做了优化，而且对许多库做了改进、合并。比如Python 2 的urllib和urllib2 在Python 3 中合并为了urllib（其实都不如request）。总之Python 3 是非常值得使用的。下面是一些安装问题，你可以去官网下载Python 3并安装，https://www.python.org/ Python 3 的Windows安装包是可以勾选path环境变量的无需手动设置。但相比这样我更推荐使用 anaconda 集成包，里面包含了许多科学包及其依赖，同样是一键安装并且可以自动配置环境变量，https://www.anaconda.com/ 提供了 Python 2，Python 3 两个版本。关于编辑器，新手可以使用Thonny，十分小巧简洁的Python 编辑器，没有复杂的工程模块，打开软件写就完了，并且自带了Python环境无需提前安装Python即可使用，也可以自己配置解释器。缺点是功能相对较弱。https://thonny.org/比较常见的是pyCharm，操作模式类似于eclipse，功能强大代，码高亮，自动填写做的很好。项目管理很方便，可以跨平台。可以很方便的管理库文件，添加外部依赖等等。提供了免费及收费两个版本。没什么大缺点，就是对外部包引入的搜索方式有时候会让人头大。明明项目里都显示有package了就是import不进来。https://www.jetbrains.com/pycharm/download/#section=windows另外推荐一个交互式笔记本，Jupyter notebook（又称IPython notebook）支持运行超过40种编程语言。安装教程十分简单。跨平台，实时代码编辑。缺点，关键词高亮很差，语法检测很弱，有时候错误语法照样可以运行。https://jupyter.org/对于热衷文本编辑器的人员可以使用sublime，另外C、Java程序员可以使用vs和eclipse。Linux推荐vim。关于同时使用Python 2 和 Python 3，如果确实需要两个环境的话。可以参考这篇文章https://www.cnblogs.com/zhengyihan1216/p/6011640.html简述一下就是安装第一个Python的时候记得把Python.exe改个别名，并且使用pip的时候要对应Python版本，最后分别添加path就可以了。下一篇介绍如何简单的使用 requests package抓取页面和表单提交。","comments":true,"tags":[]},{"title":"First Article","date":"2019-04-03T12:24:54.992Z","path":"2019/04/03/first blog/","text":"早上看了一下之前的Github实在是不忍直视还是重新制作一个吧。使用的是hexo搭建的博客，步骤比较简单。但是在themes选择上真的是强迫症患者的噩梦。偏偏就喜欢上了一个已经停止维护的yilia主题，尝试了无数次修改buffer大小之后依然无法使用HTTP下载，还是ssh吧。下载下来之后更是bug一大堆，比如tag。修改了半天总算是基本正常了。 既然是新做的博客必须好好搞一下，从网易云拉了音乐，另外搞了一下午的图灵机器人，而且失败了。。其实hexo 是自带Live2D功能的，但是想做一个高大上的带交友功能的机器人。然而node.js不熟悉挂接图灵接口的时候就是不行，最后无奈先用自带的吧。明天开始研究信息安全。有时间再搞图灵。。","comments":true,"tags":[{"name":"随笔","slug":"随笔","permalink":"https://kkkiona.github.io/tags/随笔/"}]},{"title":"Hello World","date":"2019-04-03T03:37:59.399Z","path":"2019/04/03/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","comments":true,"tags":[]}]